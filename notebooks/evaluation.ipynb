{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to evaluate model performance\n",
    "Looking at the performance of a model on the test images. \n",
    "\n",
    "**IMPORTANT!!** \n",
    "Remember to first download the models running the `download_extract_models.py` file. \n",
    "**IMPORTANT!!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys \n",
    "sys.path.append('../src/') \n",
    "from data_process import *\n",
    "from u2net import *\n",
    "from unet import *\n",
    "from resnest import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Functions displaying the results or performing different kinds of calculations in order to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(model, device, img=1):\n",
    "    \"\"\"Function displaying the predictions of a model on a given image from the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the model being evaluated\n",
    "        img (int, optional): Can take any number in range 1-10. Defaults to 1.\n",
    "    \"\"\"\n",
    "    path_to_arr = \"../data/arrays/image_{img}.npy\"\n",
    "    path_to_img = \"../data/images/image_{img}.png\"\n",
    "    mask_arr = np.load(path_to_arr)[:,:,3]\n",
    "    \n",
    "    ref = imread(path_to_img)\n",
    "    img = resize_2_256(ref)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    img = transform(img)\n",
    "    img = img.to(device)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    if model.name == \"U2NET\":\n",
    "        pred, _,_,_,_,_,_ = model(img)\n",
    "    else:\n",
    "        pred = model(img) # Rest\n",
    "\n",
    "    pred_np = pred.cpu().detach().numpy().copy()  # output_np1.shape = (1, 10, 256, 256)\n",
    "    pred_np = (np.argmax(pred_np, axis=1) * 10).astype(np.uint8)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Picture\")\n",
    "    plt.imshow(ref)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.imshow(label_2_colour(mask_arr))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted - test\")\n",
    "    plt.imshow(label_2_colour(pred_np[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the evaluation functions\n",
    "def con_matrix(ref_mask, pred_mask):\n",
    "    \"\"\"Computes the confusion matrix between a reference mask and a predicted mask.\n",
    "\n",
    "    Args:\n",
    "        ref_mask (ndarray): the reference mask\n",
    "        pred_mask (ndarray): the predicted mask\n",
    "\n",
    "    Returns:\n",
    "        ndarray: the confusion matrix\n",
    "    \"\"\"\n",
    "    true = ref_mask.reshape(-1)\n",
    "    pred = pred_mask.reshape(-1)\n",
    "    hist = confusion_matrix(true, pred)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def pixel_accuracy(hist):\n",
    "    \"\"\"Calculates the pr pixel accuracy.\"\"\"\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    return acc\n",
    "\n",
    "def mean_pixel_accuracy(hist):\n",
    "    \"\"\"Calculates the mean pixel accuracy.\"\"\"\n",
    "    acc = np.diag(hist) /  hist.sum(1)\n",
    "    mean_Acc = np.nanmean(acc)\n",
    "    return mean_Acc\n",
    "\n",
    "def miou(hist):\n",
    "    \"\"\"Calculates the mean IOU.\"\"\"\n",
    "    sum = np.mean(np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist)))\n",
    "    return sum\n",
    "\n",
    "def evaluate(model, device, test_array):\n",
    "    \"\"\"Function to evaluate a model on the test set. Return the mean IOU, mean pixel accuracy and pixel accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): a trained model (U-Net, U^2-Net, ResNeSt)\n",
    "        device (torch.device): device to run the model on\n",
    "        test_array (np.array): numpy array holding the test dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sum_miou = 0\n",
    "    sum_mean_acc = 0\n",
    "    sum_acc = 0\n",
    "    for array in test_array:\n",
    "        mask = array[:,:,3]\n",
    "        imgA = array[:,:,0:3]\n",
    "\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "        imgA = transform(imgA)\n",
    "        imgA = imgA.to(device)      \n",
    "        imgA = imgA.unsqueeze(0)\n",
    "        \n",
    "        if model.name == \"U2NET\":\n",
    "            prediction, _,_,_,_,_,_ = model(imgA)\n",
    "        else:\n",
    "            prediction = model(imgA)\n",
    "\n",
    "        pred_np = prediction.cpu().detach().numpy().copy()  \n",
    "        pred_np = (np.argmax(pred_np, axis=1) * 10).astype(np.uint8)\n",
    "        \n",
    "        hist = con_matrix(mask, pred_np[0])\n",
    "        sum_miou = miou(hist) + sum_miou\n",
    "        sum_mean_acc = mean_pixel_accuracy(hist) + sum_mean_acc\n",
    "        sum_acc = pixel_accuracy(hist) + sum_acc\n",
    "    \n",
    "    mean_miou = sum_miou / test_array.shape[0]\n",
    "    mean_acc = sum_mean_acc / test_array.shape[0]\n",
    "    acc = sum_acc / test_array.shape[0]\n",
    "    \n",
    "    return mean_miou, mean_acc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74838/1551786502.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(hist) /  hist.sum(1)\n",
      "/tmp/ipykernel_74838/1551786502.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(hist) /  hist.sum(1)\n",
      "/tmp/ipykernel_74838/1551786502.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(hist) /  hist.sum(1)\n",
      "/tmp/ipykernel_74838/1551786502.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(hist) /  hist.sum(1)\n",
      "/tmp/ipykernel_74838/1551786502.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(hist) /  hist.sum(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pixel Accuracy: 0.506\n",
      "Mean IOU: 0.391\n",
      "Pixel Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_array = np.load(\"../data/test.npy\")\n",
    "\n",
    "# Specify device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Specify which trained model to load by commenting out everyone except the one you want to evaluate\n",
    "modelname = \"unet\" \n",
    "# modelname = \"resnest50\" \n",
    "# modelname = \"resnest101\" \n",
    "# modelname = \"resnest200\" \n",
    "# modelname = \"u2net\" \n",
    "\n",
    "# Load the model\n",
    "model = torch.load(f\"../trained_models/{modelname}.pt\", map_location=device)\n",
    "\n",
    "# Evaluate\n",
    "iou, mean_acc, acc = evaluate(model, device, test_array)\n",
    "print(f\"Mean Pixel Accuracy: {round(mean_acc, 3)}\")\n",
    "print(f\"Mean IOU: {round(iou, 3)}\")\n",
    "print(f\"Pixel Accuracy: {round(acc, 3)}\")\n",
    "\n",
    "# Display the performance of the model on one of the test images\n",
    "display_results(model, device, img=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
